{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushroom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info about the data type of each column (feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the number of nulls for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# count the nulls by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for strange values. To do this, see the different values in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "n_values = [df[a].unique() for a in df.columns]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['features'] = columns\n",
    "data['n_values'] = n_values\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See how many values are in each feature, do all the features provide information? If one does not provide information, delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate between predictor variables and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target is the class column and the rest are the predictors\n",
    "y =\n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate categorical variables into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many predictor variables do we have? Is it easy to visualize them all together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, **PCA to the rescue**: just tell it how many dimensions (2D for example) you want to visualize, then it will choose those that retain more information.\n",
    "\n",
    "More info about PCA can be found at: [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TO BE FILLED: import pca library\n",
    "\n",
    "pca = #define PCA using sklearn\n",
    "pca.fit(X_train)\n",
    "\n",
    "# plot the transformed X_train in a scatterplot and color the class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will train a classifier (don't worry you will know more about it in session 4!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 50) \n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many features do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = range(2,15)\n",
    "scores = []\n",
    "\n",
    "for n in n_features:\n",
    "    # define and train PCA using n components\n",
    "    # TO BE FILLED\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = 50)\n",
    "    #Train Random Forest using the transformed X_train\n",
    "    clf.fit(# TO BE FILLED, y_train)\n",
    "    # evaluate the classifier using the transformed X_test\n",
    "    scores.append(clf.score(## TO BE FILLED,y_test))\n",
    "    \n",
    "sns.lineplot(x=n_features, y=scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from about 10 features we already have the score we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how about some clustering?\n",
    "\n",
    "**Step 1**: import the Kmeans function from sklearn <br>\n",
    "**Step 2**: find the optimal number of clusters. Hint: elbow method <br>\n",
    "**Step 3**: Generate the kmeans models, then evaluate them. Don't forget to visualize the result!\n",
    "\n",
    "[K-Means on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "#### The Elbow Method:\n",
    "One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE FILLED: import Kmeans library\n",
    "\n",
    "scores = []\n",
    "k_values = #define a range (how many clusters)\n",
    "for a in k_values:\n",
    "    # Define Kmeans and adjust\n",
    "    # TO BE FILLED\n",
    "    \n",
    "    # Save the prediction score\n",
    "    # TO BE FILLED\n",
    "sns.lineplot(x=k_values, y=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see how many clusters we need? Lets explore how well the different clusters have separated the information. To do this, a ***factorplot*** is going to be done using seaborn will do it for you. With this, we can see the distribution of the class to be predicted based on the Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Kmeans with the K value obtained\n",
    "\n",
    "kmeans = # Define and train Kmeans\n",
    "\n",
    "# Prepare the factorplot\n",
    "\n",
    "ax = sns.factorplot(col=, x=, data=, kind='count',col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the clusters. Do you remember what should we do first to visualize all the features in 2-dimensions?Let's see how this is painted. To do this, we repeat the scatterplot from before but \n",
    "***Don't forget to use the cluster assigned by kmeans as the color.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train PCA to represent\n",
    "# TO BE FILLED\n",
    "\n",
    "# Represent the clusters as color\n",
    "# TO BE FILLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
